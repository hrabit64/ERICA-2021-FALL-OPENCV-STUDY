{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 에지검출을 이용한 마스크 검출 정확도 높이기\n",
    "마스크 검출을 하기 전에 소벨 마스크로 윤곽선만 추출해서 인식 정확 높이기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "default_data_path = './data/learning/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 윤곽선 추출 이미지 가져오기\n",
    "B,G,R 각 채널들을 각각 소벨 마스크로 윤곽선 추출 후 다시 결합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def getSobelImage(mode, size):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for idx, now in enumerate(['Mask', 'Non Mask']):\n",
    "        default_path = os.path.join(default_data_path, mode, now)\n",
    "        img_name_list = os.listdir(default_path)\n",
    "\n",
    "        for img_name in img_name_list:\n",
    "            img = cv2.imread(os.path.join(default_path, img_name)) # 이미지 읽기\n",
    "            img = cv2.resize(img, size) # 이미지 224*224로 리사이즈\n",
    "            channels = cv2.split(img) # B,G,R 채널 분리\n",
    "            \n",
    "            # 각 채널마다 소벨 마스크 적용\n",
    "            for i, c in enumerate(channels):\n",
    "                channels = list(channels)\n",
    "                dx = cv2.Sobel(c, cv2.CV_64F, 1, 0, ksize=3) # x축 적용\n",
    "                dx = cv2.convertScaleAbs(dx)\n",
    "                dy = cv2.Sobel(c, cv2.CV_64F, 0, 1, ksize=3) # y축 적용\n",
    "                dy = cv2.convertScaleAbs(dy)\n",
    "                c = cv2.addWeighted(dx, 1, dy, 1, 0) # x, y축 결합\n",
    "                channels[i] = c\n",
    "            img = cv2.merge(channels) # 채널 병합\n",
    "            images.append(img)\n",
    "            labels.append(idx)\n",
    "    return np.asarray(images) / 225, np.asarray(labels) # 각 채널 225로 나누어 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.        , 0.        , 0.        ],\n",
       "         [0.72888889, 0.90666667, 0.91555556],\n",
       "         [0.71111111, 0.78222222, 0.8       ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.01777778, 0.00888889, 0.01777778],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.00888889, 0.        , 0.00888889],\n",
       "         [0.73777778, 0.90666667, 0.92444444],\n",
       "         [0.71111111, 0.78222222, 0.80888889],\n",
       "         ...,\n",
       "         [0.01777778, 0.01777778, 0.01777778],\n",
       "         [0.03555556, 0.02666667, 0.03555556],\n",
       "         [0.01777778, 0.01777778, 0.01777778]],\n",
       "\n",
       "        [[0.        , 0.        , 0.00888889],\n",
       "         [0.72888889, 0.90666667, 0.92444444],\n",
       "         [0.70222222, 0.78222222, 0.81777778],\n",
       "         ...,\n",
       "         [0.01777778, 0.01777778, 0.01777778],\n",
       "         [0.03555556, 0.02666667, 0.03555556],\n",
       "         [0.01777778, 0.01777778, 0.01777778]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.02666667, 0.02666667, 0.02666667],\n",
       "         [0.22222222, 0.18666667, 0.13333333],\n",
       "         [0.16      , 0.16      , 0.24888889],\n",
       "         ...,\n",
       "         [0.33777778, 0.34666667, 0.29333333],\n",
       "         [0.72888889, 0.75555556, 0.65777778],\n",
       "         [0.07111111, 0.08      , 0.08      ]],\n",
       "\n",
       "        [[0.02666667, 0.01777778, 0.01777778],\n",
       "         [0.20444444, 0.16      , 0.10666667],\n",
       "         [0.16      , 0.15111111, 0.24      ],\n",
       "         ...,\n",
       "         [0.35555556, 0.26666667, 0.23111111],\n",
       "         [0.96      , 0.88888889, 0.80888889],\n",
       "         [0.05333333, 0.01777778, 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.17777778, 0.14222222, 0.08888889],\n",
       "         [0.16      , 0.15111111, 0.24      ],\n",
       "         ...,\n",
       "         [0.05333333, 0.05333333, 0.        ],\n",
       "         [0.90666667, 0.90666667, 0.80888889],\n",
       "         [0.        , 0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[0.        , 0.        , 0.        ],\n",
       "         [0.12444444, 0.16      , 0.15111111],\n",
       "         [0.13333333, 0.16888889, 0.16888889],\n",
       "         ...,\n",
       "         [0.05333333, 0.05333333, 0.05333333],\n",
       "         [0.01777778, 0.01777778, 0.01777778],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.13333333, 0.15111111, 0.16888889],\n",
       "         [0.23111111, 0.31111111, 0.32      ],\n",
       "         [0.19555556, 0.28444444, 0.30222222],\n",
       "         ...,\n",
       "         [0.05333333, 0.05333333, 0.05333333],\n",
       "         [0.01777778, 0.01777778, 0.01777778],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.08888889, 0.12444444, 0.14222222],\n",
       "         [0.15111111, 0.24      , 0.25777778],\n",
       "         [0.12444444, 0.19555556, 0.22222222],\n",
       "         ...,\n",
       "         [0.05333333, 0.05333333, 0.05333333],\n",
       "         [0.01777778, 0.01777778, 0.01777778],\n",
       "         [0.00888889, 0.00888889, 0.00888889]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.12444444, 0.09777778, 0.15111111],\n",
       "         [0.19555556, 0.23111111, 0.33777778],\n",
       "         [0.15111111, 0.16      , 0.30222222],\n",
       "         ...,\n",
       "         [0.08888889, 0.07111111, 0.07111111],\n",
       "         [0.05333333, 0.05333333, 0.05333333],\n",
       "         [0.00888889, 0.00888889, 0.00888889]],\n",
       "\n",
       "        [[0.10666667, 0.08888889, 0.13333333],\n",
       "         [0.22222222, 0.26666667, 0.36444444],\n",
       "         [0.16888889, 0.19555556, 0.31111111],\n",
       "         ...,\n",
       "         [0.08888889, 0.08888889, 0.08      ],\n",
       "         [0.04444444, 0.04444444, 0.04444444],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.12444444, 0.17777778, 0.24888889],\n",
       "         [0.08      , 0.11555556, 0.21333333],\n",
       "         ...,\n",
       "         [0.05333333, 0.06222222, 0.04444444],\n",
       "         [0.02666667, 0.02666667, 0.02666667],\n",
       "         [0.        , 0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[0.        , 0.        , 0.        ],\n",
       "         [0.00888889, 0.00888889, 0.00888889],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.05333333, 0.01777778, 0.08      ],\n",
       "         [0.04444444, 0.07111111, 0.01777778],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.01777778, 0.01777778, 0.01777778],\n",
       "         [0.00888889, 0.00888889, 0.00888889],\n",
       "         [0.01777778, 0.01777778, 0.01777778],\n",
       "         ...,\n",
       "         [0.08888889, 0.05333333, 0.18666667],\n",
       "         [0.08      , 0.08888889, 0.09777778],\n",
       "         [0.05333333, 0.01777778, 0.08      ]],\n",
       "\n",
       "        [[0.07111111, 0.07111111, 0.07111111],\n",
       "         [0.08888889, 0.08888889, 0.08888889],\n",
       "         [0.11555556, 0.11555556, 0.11555556],\n",
       "         ...,\n",
       "         [0.10666667, 0.17777778, 0.24      ],\n",
       "         [0.08888889, 0.18666667, 0.18666667],\n",
       "         [0.07111111, 0.14222222, 0.14222222]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.06222222, 0.06222222, 0.06222222],\n",
       "         [0.13333333, 0.12444444, 0.13333333],\n",
       "         [0.08888889, 0.08888889, 0.08888889],\n",
       "         ...,\n",
       "         [1.04      , 1.03111111, 1.04888889],\n",
       "         [0.10666667, 0.09777778, 0.09777778],\n",
       "         [0.01777778, 0.05333333, 0.04444444]],\n",
       "\n",
       "        [[0.02666667, 0.03555556, 0.02666667],\n",
       "         [0.12444444, 0.12444444, 0.12444444],\n",
       "         [0.10666667, 0.12444444, 0.08      ],\n",
       "         ...,\n",
       "         [0.20444444, 0.15111111, 0.16      ],\n",
       "         [0.15111111, 0.02666667, 0.05333333],\n",
       "         [0.06222222, 0.01777778, 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.08888889, 0.07111111, 0.11555556],\n",
       "         [0.07111111, 0.07111111, 0.07111111],\n",
       "         ...,\n",
       "         [0.03555556, 0.08      , 0.07111111],\n",
       "         [0.01777778, 0.04444444, 0.03555556],\n",
       "         [0.        , 0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.00888889, 0.00888889, 0.00888889],\n",
       "         [0.00888889, 0.00888889, 0.00888889],\n",
       "         [0.01777778, 0.01777778, 0.01777778],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.00888889, 0.00888889, 0.00888889],\n",
       "         [0.05333333, 0.05333333, 0.05333333],\n",
       "         [0.21333333, 0.21333333, 0.21333333],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.03555556, 0.        , 0.17777778],\n",
       "         [0.59555556, 0.28444444, 0.34666667],\n",
       "         [0.81777778, 0.80888889, 0.71111111],\n",
       "         ...,\n",
       "         [0.04444444, 0.04444444, 0.00888889],\n",
       "         [0.08888889, 0.01777778, 0.05333333],\n",
       "         [0.02666667, 0.00888889, 0.00888889]],\n",
       "\n",
       "        [[0.47111111, 0.44444444, 0.52444444],\n",
       "         [0.88      , 0.67555556, 0.70222222],\n",
       "         [1.10222222, 1.13333333, 1.13333333],\n",
       "         ...,\n",
       "         [0.06222222, 0.06222222, 0.01777778],\n",
       "         [0.09777778, 0.04444444, 0.06222222],\n",
       "         [0.01777778, 0.02666667, 0.01777778]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.40888889, 0.23111111, 0.17777778],\n",
       "         [0.25777778, 0.32      , 0.4       ],\n",
       "         ...,\n",
       "         [0.06222222, 0.05333333, 0.        ],\n",
       "         [0.08888889, 0.02666667, 0.04444444],\n",
       "         [0.        , 0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[0.        , 0.        , 0.        ],\n",
       "         [0.06222222, 0.04444444, 0.04444444],\n",
       "         [0.06222222, 0.03555556, 0.04444444],\n",
       "         ...,\n",
       "         [0.02666667, 0.08888889, 0.13333333],\n",
       "         [0.22222222, 0.24888889, 0.22222222],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.02666667, 0.15111111, 0.21333333],\n",
       "         [0.08888889, 0.19555556, 0.25777778],\n",
       "         [0.11555556, 0.14222222, 0.17777778],\n",
       "         ...,\n",
       "         [0.26666667, 0.27555556, 0.36444444],\n",
       "         [0.46222222, 0.39111111, 0.44444444],\n",
       "         [0.50666667, 0.34666667, 0.42666667]],\n",
       "\n",
       "        [[0.19555556, 0.36444444, 0.47111111],\n",
       "         [0.30222222, 0.45333333, 0.59555556],\n",
       "         [0.24888889, 0.4       , 0.50666667],\n",
       "         ...,\n",
       "         [0.55111111, 0.41777778, 0.41777778],\n",
       "         [0.67555556, 0.48888889, 0.48      ],\n",
       "         [0.63111111, 0.53333333, 0.49777778]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.00888889, 0.        , 0.04444444],\n",
       "         [0.02666667, 0.03555556, 0.05333333],\n",
       "         [0.02666667, 0.03555556, 0.05333333],\n",
       "         ...,\n",
       "         [0.43555556, 1.04      , 1.13333333],\n",
       "         [0.47111111, 0.87111111, 1.13333333],\n",
       "         [0.24888889, 0.44444444, 0.89777778]],\n",
       "\n",
       "        [[0.01777778, 0.        , 0.05333333],\n",
       "         [0.02666667, 0.03555556, 0.05333333],\n",
       "         [0.01777778, 0.02666667, 0.05333333],\n",
       "         ...,\n",
       "         [0.25777778, 0.81777778, 1.13333333],\n",
       "         [0.29333333, 0.51555556, 0.83555556],\n",
       "         [0.24      , 0.27555556, 0.56      ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.00888889, 0.03555556, 0.00888889],\n",
       "         [0.00888889, 0.02666667, 0.03555556],\n",
       "         ...,\n",
       "         [0.00888889, 0.04444444, 0.13333333],\n",
       "         [0.05333333, 0.06222222, 0.12444444],\n",
       "         [0.        , 0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[0.        , 0.        , 0.        ],\n",
       "         [0.01777778, 0.12444444, 0.06222222],\n",
       "         [0.12444444, 0.16888889, 0.11555556],\n",
       "         ...,\n",
       "         [0.28444444, 0.21333333, 0.23111111],\n",
       "         [0.14222222, 0.05333333, 0.07111111],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.30222222, 0.08888889, 0.06222222],\n",
       "         [0.32      , 0.21333333, 0.12444444],\n",
       "         [0.30222222, 0.25777778, 0.16      ],\n",
       "         ...,\n",
       "         [0.78222222, 0.57777778, 0.48      ],\n",
       "         [0.69333333, 0.48888889, 0.37333333],\n",
       "         [0.51555556, 0.42666667, 0.27555556]],\n",
       "\n",
       "        [[0.22222222, 0.17777778, 0.08      ],\n",
       "         [0.36444444, 0.30222222, 0.16      ],\n",
       "         [0.09777778, 0.17777778, 0.08      ],\n",
       "         ...,\n",
       "         [0.20444444, 0.27555556, 0.31111111],\n",
       "         [0.31111111, 0.29333333, 0.32      ],\n",
       "         [0.07111111, 0.07111111, 0.05333333]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.        , 0.00888889, 0.00888889],\n",
       "         [0.01777778, 0.01777778, 0.01777778],\n",
       "         [0.01777778, 0.00888889, 0.00888889],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.00888889, 0.00888889, 0.02666667],\n",
       "         [0.        , 0.        , 0.00888889]],\n",
       "\n",
       "        [[0.02666667, 0.        , 0.        ],\n",
       "         [0.00888889, 0.01777778, 0.01777778],\n",
       "         [0.01777778, 0.01777778, 0.01777778],\n",
       "         ...,\n",
       "         [0.00888889, 0.00888889, 0.00888889],\n",
       "         [0.00888889, 0.01777778, 0.02666667],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.01777778, 0.        , 0.        ],\n",
       "         [0.00888889, 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.01777778, 0.01777778],\n",
       "         [0.        , 0.        , 0.        ]]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_sobel, train_y_sobel = getSobelImage('Train', (224,224))\n",
    "val_data_sobel, val_y_sobel = getSobelImage('Validation', (224,224))\n",
    "train_data_sobel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기학습된 VGG16 전이학습\n",
    "VGG16모델 부분은 가중치를 고정시켜두고, 출력부분에 fully connected layer를 추가해 이 부분만 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vgg16():\n",
    "    # VGG16모델 불러옴\n",
    "    pre_trained_vgg = keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    pre_trained_vgg.trainable = False # weight 고정\n",
    "    \n",
    "    # fully connected 모델 구현\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(pre_trained_vgg)\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(1024, kernel_regularizer=keras.regularizers.l1_l2(l1=0.001,l2=0.001), activation='relu'))\n",
    "    model.add(keras.layers.Dropout(0.5))\n",
    "    model.add(keras.layers.Dense(512, kernel_regularizer=keras.regularizers.l1_l2(l1=0.001,l2=0.001), activation='relu'))\n",
    "    model.add(keras.layers.Dropout(0.5))\n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid')) # 0, 1 두 값으로 나오게 sigmoid 함수 사용\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sobel = get_vgg16()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 컴파일 후 학습\n",
    "모델 컴파일 후 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 25088)             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1024)              25691136  \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,931,137\n",
      "Trainable params: 26,216,449\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_sobel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_sobel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-24 03:22:20.808066: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8101\n",
      "2021-11-24 03:22:21.490665: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 898.00MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2021-11-24 03:22:21.490714: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 898.00MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2021-11-24 03:22:21.490749: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.28GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2021-11-24 03:22:21.490777: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.28GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2021-11-24 03:22:21.562425: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 898.56MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2021-11-24 03:22:21.562454: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 898.56MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2021-11-24 03:22:21.771759: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.09GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2021-11-24 03:22:21.771784: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.09GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2021-11-24 03:22:22.226893: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.17GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2021-11-24 03:22:22.226918: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.17GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 16s 160ms/step - loss: 40.2950 - accuracy: 0.6267 - val_loss: 21.3334 - val_accuracy: 0.5033\n",
      "Epoch 2/50\n",
      "75/75 [==============================] - 13s 172ms/step - loss: 18.1431 - accuracy: 0.7000 - val_loss: 16.1599 - val_accuracy: 0.5229\n",
      "Epoch 3/50\n",
      "75/75 [==============================] - 13s 174ms/step - loss: 15.4317 - accuracy: 0.7333 - val_loss: 14.3890 - val_accuracy: 0.8954\n",
      "Epoch 4/50\n",
      "75/75 [==============================] - 13s 174ms/step - loss: 14.5500 - accuracy: 0.7583 - val_loss: 14.0779 - val_accuracy: 0.8366\n",
      "Epoch 5/50\n",
      "75/75 [==============================] - 13s 173ms/step - loss: 14.1902 - accuracy: 0.7950 - val_loss: 14.0114 - val_accuracy: 0.8987\n",
      "Epoch 6/50\n",
      "75/75 [==============================] - 13s 172ms/step - loss: 14.0250 - accuracy: 0.8100 - val_loss: 13.7243 - val_accuracy: 0.9118\n",
      "Epoch 7/50\n",
      "75/75 [==============================] - 13s 173ms/step - loss: 13.9223 - accuracy: 0.8267 - val_loss: 13.8853 - val_accuracy: 0.8529\n",
      "Epoch 8/50\n",
      "75/75 [==============================] - 13s 173ms/step - loss: 13.8401 - accuracy: 0.8517 - val_loss: 14.0759 - val_accuracy: 0.7549\n",
      "Epoch 9/50\n",
      "75/75 [==============================] - 10s 139ms/step - loss: 13.8537 - accuracy: 0.8483 - val_loss: 13.8040 - val_accuracy: 0.8268\n",
      "Epoch 10/50\n",
      "75/75 [==============================] - 10s 139ms/step - loss: 13.8397 - accuracy: 0.8417 - val_loss: 14.2671 - val_accuracy: 0.6830\n",
      "Epoch 11/50\n",
      "75/75 [==============================] - 10s 139ms/step - loss: 13.8339 - accuracy: 0.8383 - val_loss: 13.6643 - val_accuracy: 0.9085\n",
      "Epoch 12/50\n",
      "75/75 [==============================] - 13s 175ms/step - loss: 13.8215 - accuracy: 0.8550 - val_loss: 13.6554 - val_accuracy: 0.9183\n",
      "Epoch 13/50\n",
      "75/75 [==============================] - 10s 139ms/step - loss: 13.8066 - accuracy: 0.8450 - val_loss: 13.9737 - val_accuracy: 0.7484\n",
      "Epoch 14/50\n",
      "75/75 [==============================] - 10s 139ms/step - loss: 13.7922 - accuracy: 0.8500 - val_loss: 14.5736 - val_accuracy: 0.6111\n",
      "Epoch 15/50\n",
      "75/75 [==============================] - 10s 139ms/step - loss: 13.7700 - accuracy: 0.8467 - val_loss: 13.6801 - val_accuracy: 0.8889\n",
      "Epoch 16/50\n",
      "75/75 [==============================] - 13s 174ms/step - loss: 13.7497 - accuracy: 0.8633 - val_loss: 13.6713 - val_accuracy: 0.9020\n",
      "Epoch 17/50\n",
      "75/75 [==============================] - 10s 139ms/step - loss: 13.7790 - accuracy: 0.8467 - val_loss: 13.6865 - val_accuracy: 0.9020\n",
      "Epoch 18/50\n",
      "75/75 [==============================] - 13s 174ms/step - loss: 13.7513 - accuracy: 0.8717 - val_loss: 13.5633 - val_accuracy: 0.9216\n",
      "Epoch 19/50\n",
      "75/75 [==============================] - 13s 174ms/step - loss: 13.7171 - accuracy: 0.8850 - val_loss: 13.6717 - val_accuracy: 0.9118\n",
      "Epoch 20/50\n",
      "75/75 [==============================] - 10s 139ms/step - loss: 13.7784 - accuracy: 0.8483 - val_loss: 13.7946 - val_accuracy: 0.8399\n",
      "Epoch 21/50\n",
      "75/75 [==============================] - 10s 139ms/step - loss: 13.7583 - accuracy: 0.8700 - val_loss: 13.7455 - val_accuracy: 0.8333\n",
      "Epoch 22/50\n",
      "75/75 [==============================] - 10s 139ms/step - loss: 13.7186 - accuracy: 0.8683 - val_loss: 14.1787 - val_accuracy: 0.6928\n",
      "Epoch 23/50\n",
      "75/75 [==============================] - 10s 139ms/step - loss: 13.7581 - accuracy: 0.8500 - val_loss: 13.8038 - val_accuracy: 0.8595\n",
      "Epoch 24/50\n",
      "75/75 [==============================] - 10s 139ms/step - loss: 13.7796 - accuracy: 0.8550 - val_loss: 13.9763 - val_accuracy: 0.6993\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc95823e550>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [keras.callbacks.EarlyStopping(monitor='accuracy', mode='max', patience=5),\n",
    "                 keras.callbacks.ModelCheckpoint(filepath='./data/model/best_model_sobel.h5', monitor='accuracy', save_best_only=True)]\n",
    "model_sobel.fit(train_data_sobel, train_y_sobel, epochs=50, batch_size=8,\n",
    "                    validation_data=(val_data_sobel, val_y_sobel), shuffle=True, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 아무 전처리도 하지 않은 사진 학습\n",
    "학습 방법은 위와 같음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImage(mode, size):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for idx, now in enumerate(['Mask', 'Non Mask']):\n",
    "        default_path = os.path.join(default_data_path, mode, now)\n",
    "        img_name_list = os.listdir(default_path)\n",
    "\n",
    "        for img_name in img_name_list:\n",
    "            img = cv2.imread(os.path.join(default_path, img_name))\n",
    "            img = cv2.resize(img, size)\n",
    "            images.append(img)\n",
    "            labels.append(idx)\n",
    "    return np.asarray(images) / 225, np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-24 03:38:50.166478: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8101\n",
      "2021-11-24 03:38:50.832364: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 898.00MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2021-11-24 03:38:50.832411: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 898.00MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2021-11-24 03:38:50.832447: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.28GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2021-11-24 03:38:50.832464: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.28GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2021-11-24 03:38:50.891600: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 898.56MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2021-11-24 03:38:50.891625: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 898.56MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2021-11-24 03:38:51.101329: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.09GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2021-11-24 03:38:51.101353: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.09GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2021-11-24 03:38:51.553388: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.17GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2021-11-24 03:38:51.553412: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.17GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 18s 190ms/step - loss: 39.9828 - accuracy: 0.7117 - val_loss: 19.2971 - val_accuracy: 0.6961\n",
      "Epoch 2/50\n",
      "75/75 [==============================] - 13s 172ms/step - loss: 17.6177 - accuracy: 0.7767 - val_loss: 15.7407 - val_accuracy: 0.7386\n",
      "Epoch 3/50\n",
      "75/75 [==============================] - 13s 174ms/step - loss: 15.0171 - accuracy: 0.8383 - val_loss: 14.6664 - val_accuracy: 0.9641\n",
      "Epoch 4/50\n",
      "75/75 [==============================] - 13s 172ms/step - loss: 14.2527 - accuracy: 0.8550 - val_loss: 13.8301 - val_accuracy: 0.9346\n",
      "Epoch 5/50\n",
      "75/75 [==============================] - 10s 139ms/step - loss: 14.0582 - accuracy: 0.8550 - val_loss: 15.9518 - val_accuracy: 0.5000\n",
      "Epoch 6/50\n",
      "75/75 [==============================] - 13s 173ms/step - loss: 13.9357 - accuracy: 0.8717 - val_loss: 13.5063 - val_accuracy: 0.9608\n",
      "Epoch 7/50\n",
      "75/75 [==============================] - 13s 174ms/step - loss: 13.7629 - accuracy: 0.8967 - val_loss: 13.6128 - val_accuracy: 0.9739\n",
      "Epoch 8/50\n",
      "75/75 [==============================] - 10s 139ms/step - loss: 13.7424 - accuracy: 0.8950 - val_loss: 13.7408 - val_accuracy: 0.8922\n",
      "Epoch 9/50\n",
      "75/75 [==============================] - 10s 139ms/step - loss: 13.7438 - accuracy: 0.8683 - val_loss: 13.6599 - val_accuracy: 0.9739\n",
      "Epoch 10/50\n",
      "75/75 [==============================] - 10s 139ms/step - loss: 13.7368 - accuracy: 0.8883 - val_loss: 13.3995 - val_accuracy: 0.9739\n",
      "Epoch 11/50\n",
      "75/75 [==============================] - 10s 139ms/step - loss: 13.7013 - accuracy: 0.8833 - val_loss: 13.5603 - val_accuracy: 0.9673\n",
      "Epoch 12/50\n",
      "75/75 [==============================] - 10s 139ms/step - loss: 13.7093 - accuracy: 0.8800 - val_loss: 13.7333 - val_accuracy: 0.8987\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc0af7bca00>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, train_y = getImage('Train', (224,224))\n",
    "val_data, val_y = getImage('Validation', (224,224))\n",
    "\n",
    "model = get_vgg16()\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "callbacks = [keras.callbacks.EarlyStopping(monitor='accuracy', mode='max', patience=5),\n",
    "                 keras.callbacks.ModelCheckpoint(filepath='./data/model/best_model.h5', monitor='accuracy', save_best_only=True)]\n",
    "model.fit(train_data, train_y, epochs=50, batch_size=8,\n",
    "                    validation_data=(val_data, val_y), shuffle=True, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 두 모델 성능 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_sobel, test_y_sobel = getSobelImage('Test', (224, 224))\n",
    "test_data, test_y = getImage('Test', (224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sobel = keras.models.load_model('./data/model/best_model_sobel.h5')\n",
    "model = keras.models.load_model('./data/model/best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 152ms/step - loss: 13.6040 - accuracy: 0.8800\n",
      "소벨 모델 정확도: 0.8799999952316284\n",
      "4/4 [==============================] - 1s 153ms/step - loss: 13.8823 - accuracy: 0.7800\n",
      "일반 모델 정확도: 0.7799999713897705\n"
     ]
    }
   ],
   "source": [
    "print('소벨 모델 정확도:', model_sobel.evaluate(test_data_sobel, test_y_sobel)[1])\n",
    "print('일반 모델 정확도:', model.evaluate(test_data, test_y)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "[레포 보러가기](https://github.com/K1A2/mask_detection_vgg16_sobel/tree/master)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}